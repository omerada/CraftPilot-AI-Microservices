server:
  port: ${SERVER_PORT:8081}
  compression:
    enabled: true
    mime-types: application/json,text/plain
    min-response-size: 1024
  shutdown: graceful
  netty:
    connection-timeout: 5000
    idle-timeout: 30000
    max-keep-alive-requests: 1000
    threads:
      worker: 4
      boss: 1
    validate-headers: true
    max-connections: 10000
    max-initial-line-length: 8192
    max-header-size: 16384
  tomcat:
    connection-timeout: 60000
    max-connections: 1000
    max-keep-alive-requests: 1000
    threads:
      max: 200
      min-spare: 20
  error:
    include-message: always
    include-binding-errors: always
    include-stacktrace: never
    include-exception: false

spring:
  application:
    name: llm-service
  profiles:
    active: ${SPRING_PROFILES_ACTIVE:dev}
    include: kafka-base
  main:
    web-application-type: reactive
    allow-bean-definition-overriding: true
  webflux:
    base-path: /
    format:
      date: yyyy-MM-dd
      date-time: yyyy-MM-dd HH:mm:ss
      time: HH:mm:ss
    log-request-details: true
  codec:
    max-in-memory-size: 16MB
    log-request-details: true
  kafka:
    properties:
      security.protocol: PLAINTEXT
      reconnect.backoff.ms: 1000
      reconnect.backoff.max.ms: 5000
    consumer:
      auto-offset-reset: earliest
      group-id: llm-service-group
    producer:
      retries: 3
      acks: all
  cloud:
    discovery:
      enabled: true
  data:
    redis:
      host: ${REDIS_HOST:redis}
      port: ${REDIS_PORT:6379}
      password: ${REDIS_PASSWORD:13579ada}
      timeout: 5000
      lettuce:
        pool:
          max-active: 8
          max-idle: 8
          min-idle: 2
          max-wait: 1000ms
  cache:
    type: redis
    redis:
      time-to-live: 3600000
      cache-null-values: false
      use-key-prefix: true
  reactor:
    netty:
      pool:
        type: elastic
        maxConnections: 500
        acquireTimeout: 5000
      client:
        proxy:
          type: NO_PROXY
  security:
    basic:
      enabled: false
  netty:
    connection-timeout: 60000
  mvc:
    async:
      request-timeout: 60000

eureka:
  client:
    serviceUrl:
      defaultZone: ${EUREKA_CLIENT_SERVICEURL_DEFAULTZONE:http://craftpilot:13579ada@eureka-server:8761/eureka/}
    register-with-eureka: true
    fetch-registry: true
  instance:
    prefer-ip-address: false
    hostname: ${HOSTNAME:llm-service}
    lease-renewal-interval-in-seconds: 10
    health-check-url-path: /actuator/health

management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics
      base-path: /actuator
  endpoint:
    health:
      show-details: always
      probes:
        enabled: true
      group:
        readiness:
          include: "*"
        liveness:
          include: "*"
  health:
    livenessstate:
      enabled: true
    readinessstate:
      enabled: true

openrouter:
  api:
    key: ${OPENROUTER_API_KEY}
    url: https://openrouter.ai/api/v1
    defaultModel: google/gemini-2.0-flash-lite-preview-02-05:free
    maxTokens: 2000
    temperature: 0.7
    retryAttempts: 3
    retryDelay: 1000
    http-referer: https://craftpilot.io

kafka:
  topics:
    ai-events: ai-events
    llm-completions: llm-completions

logging:
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] [%X{traceId}] %-5level %logger{36} - %msg%n"
  level:
    root: INFO
    com.craftpilot: DEBUG
    org.springframework.web: DEBUG
    org.springframework.cloud: DEBUG
    reactor.netty: INFO
    io.netty: DEBUG
    com.craftpilot.llmservice: DEBUG
    org.springframework.security: INFO

netty:
  native:
    enabled: false
  io:
    noNative: true
    noUnsafe: true
  use-native-transport: true
  workdir: /tmp/netty

resilience4j:
  circuitbreaker:
    instances:
      llmService:
        slidingWindowSize: 100
        failureRateThreshold: 50
        waitDurationInOpenState: 20s
        permittedNumberOfCallsInHalfOpenState: 10
  ratelimiter:
    instances:
      llmService:
        limitForPeriod: 100
        limitRefreshPeriod: 1s
        timeoutDuration: 5s
  bulkhead:
    instances:
      llmService:
        maxConcurrentCalls: 50
